### Solutions Engineer Hiring Challenge Answers - Parth Parekh

### Setting up the environment

Utlized Vagrant Ubuntu virtual machine for solving the challenge. Working with vagrant is extremely easy as you need to just import the pre-made images or boxes. Then created sign up in data dog with following user ID and downloaded the Datadog agent for Ubuntu.   

![capture1](https://user-images.githubusercontent.com/40180603/48367908-270a7800-e680-11e8-9043-a84077569a80.JPG)

---
### Collecting Metrics :

#### 1) Adding Tags
1.1) For adding tags, I went into the datadog.yaml which is an agent configuration file located at _/etc/datadog-agent/_ directory. I modified the file and added the tags to it.
![tags1](https://user-images.githubusercontent.com/40180603/48451942-cf9d0280-e77a-11e8-90c8-bb67de297e2b.JPG)



1.2) After restarting the agent, observed the following output


![tag_names_host](https://user-images.githubusercontent.com/40180603/48452358-b5fcba80-e77c-11e8-9d18-6962c93f8753.jpg)




#### 2) Adding Database Integration

2.1) After successful downloading PostgreSQL into the ubuntu, went to _Datadog application -> Integrations -> Configurations_.

2.2) In Ubuntu, started the psql server and created Datadog user to grant proper access to PostgresSQL server with following commands
```
create user datadog with password '<password generated by system>';
grant SELECT ON pg_stat_database to datadog;
```
```
psql -h localhost -U datadog postgres -c "select * from pg_stat_database LIMIT(1);" && \
echo -e "\e[0;32mPostgres connection - OK\e[0m" || \
echo -e "\e[0;31mCannot connect to Postgres\e[0m"
```
2.3) Inputted the passwords where required.

2.4) Modified the _postgre.yaml_ file present in the _conf.d_ folder and added following code to it.

![postgresyaml](https://user-images.githubusercontent.com/40180603/48442081-22ff5880-e75b-11e8-8aae-a13dcbceacb0.JPG)

2.5) Checked the status of the Datadog agent to verify whether the integration checked has been passed and observed the following output

![postgres_capture](https://user-images.githubusercontent.com/40180603/48374771-d735ac00-e693-11e8-84a4-bc4850a26f04.JPG)

The integration was successfully installed

![postgres_inte](https://user-images.githubusercontent.com/40180603/48374910-2a0f6380-e694-11e8-9039-acde5f777615.JPG)

There is another way to verify whether integration is successful is by checking in the Timeboard and you will be able to see all PostgreSQL metrices

![ver_postgres](https://user-images.githubusercontent.com/40180603/48375151-04368e80-e695-11e8-909e-271a6e72264c.jpg)


#### 3) Creating a custom Agent check that submits a metric named my_metric

3.1) Went into _checks.d_ folder and created the new file _MyCheck.py_ containing following code

```
from random import randint

try:
# first, try to import the base class from old versions of the Agent...
 from checks import AgentCheck
except ImportError:
 # ...if the above failed, the check is running in Agent version 6 or later
 from datadog_checks.checks import AgentCheck

# content of the special variable __version__ will be shown in the Agent status page

__version__ = "1.0.0"

class MyCheck(AgentCheck):
   def check(self, instance):
       self.gauge('my_metric', randint(0, 1000))
```
Here name of the metric generated is _'my_metric'_ which generates random values between 0 to 1000 using randint function.


#### 4) Changing check's collection interval to 45 secs

4.1) In conf.d folder and created _MyCheck.yaml_ file with following code to set _minimum collection interval_ to 45 seconds. The name of both .yaml and .py file should be the same.

```
init_config:

instances:
     - min_collection_interval: 45
```
On checking the status of the agent we can see that the metric has been created 
![mycheck_metricjpg](https://user-images.githubusercontent.com/40180603/48376668-993b8680-e699-11e8-8384-d93da34334f1.JPG)

---
### Visualizing Data :
1) Generated my API and APM keys from the Datadog UI. These keys can be found in the _Integrations tab -> APIs_. 

2) Created _My_Graph.py_ file containing the following code.
https://github.com/Parth9000/Datadogfiles/blob/master/My_Graph.py

After running the _My_Graph.py_ file, went into my Datadog UI and following timeboards were generated


3) Custom metric scoped over your host
![viz1](https://user-images.githubusercontent.com/40180603/48387341-6e662800-e6c3-11e8-8c97-4c7d70a996e1.JPG)

4) Metric from the Integration on your Database with the anomaly function
![viz2](https://user-images.githubusercontent.com/40180603/48387381-92296e00-e6c3-11e8-8611-ba800156d124.JPG)


5) Custom metric with the rollup function applied to sum up all the points for the past hour
![viz3](https://user-images.githubusercontent.com/40180603/48387420-c00eb280-e6c3-11e8-8970-7d319fff3e93.JPG)

The entire dashboard looks like this

![viz4](https://user-images.githubusercontent.com/40180603/48387467-fb10e600-e6c3-11e8-9d01-2a86b4d0065d.JPG)

6) Created a UI interval for 5 minutes by going into the dashboard and clicked the 'Annotate this Graph' icon
![viz7](https://user-images.githubusercontent.com/40180603/48387766-17f9e900-e6c5-11e8-9d8c-ef4763c7b130.JPG)

7) In the comments section added '@' followed by my name, to send it to me
![viz5](https://user-images.githubusercontent.com/40180603/48442215-8e492a80-e75b-11e8-8e98-ce08be5eb393.JPG)


8) The Anomaly graph is describing the write time of the Postgresql. Since there were no operations to the database, it remained constant. As soon as there were some operations or query to the database, the write metrics were generated and the graph reported the spike. As this spike is an Anomaly, it is red in color.  
---
### Monitoring Data:

#### 1) Creating Monitors

1.1) Utilized Datadog UI for creating the metric. Went into Monitors -> New Monitor -> Metric

1.2) Selected the detection method as Threshold Alert. Inputted _my_metric_ in 'Define the Metric' and created a multialert for the host.

![monitor1](https://user-images.githubusercontent.com/40180603/48393235-bd6c8700-e6dc-11e8-958f-6100ee796f0d.jpg)


1.3) Inputted Warning threshold as 500, Alerting threshold as 800 and set the duration to monitor the _my_metric_ to 5 minutes. I also set the monitor to notify of the data is missing for 10 minutes.

![monitor2](https://user-images.githubusercontent.com/40180603/48393277-e4c35400-e6dc-11e8-9db0-52afd7c9a1c2.jpg)


1.4) In the below image we can see that for each alert, warning, no data, we can set different messages to notify us. For email notification, I wrote my personal email ID in the 'Notify your Team'. Any variable can be written inside {{}}. For example, to know the host ip we can write {{host.ip}} in the messages.

![monitor3](https://user-images.githubusercontent.com/40180603/48393322-13412f00-e6dd-11e8-86fb-08f5b8514dd3.jpg)



The alert email sent is shown below. As we can see that the value of _host.ip_ and average of _my_metric_ over last 5 mins is displayed in the email.

![monitor4](https://user-images.githubusercontent.com/40180603/48393357-2ce27680-e6dd-11e8-9137-86271027b922.JPG)

#### 2) Scheduling Downtime

2.1) For Scheduling the Downtime, went into Datadog _UI -> Monitors -> Manage downtime -> Schedule Downtime_. Created two schedule one for Monday to Friday 7pm to 9am and other for Weekends.



2.2) Selected the monitor for which downtime needs to be created. Inputted the start time at 7pm and kept the duration for 14 hours, which will be next day's 9 am. Kept the 'Repeat Every' value to 1 days to schedule the downtime for all the days. Inserted my email ID on the 'Notify your Team' to get the notification.  

![downtime1](https://user-images.githubusercontent.com/40180603/48394386-64ebb880-e6e1-11e8-807c-50aba2b53b67.jpg)


2.3) Created the Second Schedule for the Weekends.For this, I set 'Repeat Every' value to 'weeks' and selected Saturday and Sunday. Kept the duration for 24 hours which is equivalent to 1 day.


![downtime2](https://user-images.githubusercontent.com/40180603/48394433-8ea4df80-e6e1-11e8-8e67-4bb7bf80264c.jpg)


2.4) Recievd the following two emails on creation of downtimes

![downtime4](https://user-images.githubusercontent.com/40180603/48394462-aa0fea80-e6e1-11e8-99ff-c10238d329c7.JPG)

![downtime3](https://user-images.githubusercontent.com/40180603/48394486-c2800500-e6e1-11e8-82f9-e1c0fc16d727.JPG)





---
### Collecting APM Data:
1) Created _My_App.py_ flask file in ubuntu with the following code. This is the application that is to be traced. 
```

from flask import Flask
import logging
import sys

# Have flask use stdout as the logger
main_logger = logging.getLogger()
main_logger.setLevel(logging.DEBUG)
c = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
c.setFormatter(formatter)
main_logger.addHandler(c)

app = Flask(__name__)

@app.route('/')
def api_entry():
    return 'Entrypoint to the Application'

@app.route('/api/apm')
def apm_endpoint():
    return 'Getting APM Started'

@app.route('/api/trace')
def trace_endpoint():
    return 'Posting Traces'

if __name__ == '__main__':
    app.run(host='0.0.0.0', port='5050')
 ```   
 link for the flask app file : https://github.com/Parth9000/Datadogfiles/blob/master/My_App.py
 
2) Enabled the trace collection by the setting the apm_config key value to true in _datadog.yaml_ file. Through this step we give permission to trace.
![trace_conf](https://user-images.githubusercontent.com/40180603/48382761-98fab580-e6b0-11e8-8ea6-feb49f9b5d88.JPG)


3) Downloaded the _ddtrace_ for python using ``` pip install ddtrace```. Datadog agent traces the application through ddtrace
 

4) Ran the ddtrace for the _my_app.py_ file using ```ddtrace-run python My_App.py```

Obtained the following output

![apm1](https://user-images.githubusercontent.com/40180603/48382810-e70fb900-e6b0-11e8-9e14-dc4c8628b805.JPG)


![apm2](https://user-images.githubusercontent.com/40180603/48382833-fd1d7980-e6b0-11e8-8823-8cd69d9c4bcd.JPG)

5) Difference Between Service and a Resource

A Service is a set of processes that work together to provide a feature or output in an application. Resource is a query or an action to a particular service. 

![apm3](https://user-images.githubusercontent.com/40180603/48383793-4b347c00-e6b5-11e8-81dc-249c6c90634f.jpg)

In the above image, I can see Flask is a service and there are two resources of that service, _apm_endpoint_ and _api_entry_. If database is one form of the service the SQL query to that database will be the resource. 

6) Public URL and screenshot of the Dashboard URL of the APM and Infrastructure metrics

https://p.datadoghq.com/sb/b16cd2a55-a7c61fef6f9564347bf3fa0850729ffc

![apm4](https://user-images.githubusercontent.com/40180603/48442678-dddc2600-e75c-11e8-8299-3d93c4a1b05d.JPG)



---
### Final Question

We are surrounded by data. A employee needs to look at various parameters in order to make a decision. With Datadog monitoring real time applications and systems, it is also important for employees to track other real time data such as news and stock prices. If this data is available at a single dasboard or application, then it is really easy to make a decision. 

I have created a python file that sends stock price of Amazon Inc as an event

https://github.com/Parth9000/Datadogfiles/blob/master/My_Price.py

The event generated is as follows

![final1](https://user-images.githubusercontent.com/40180603/48399059-9455f180-e6f0-11e8-996d-f53cb7dea8d0.JPG)

Similarly Datadog functionality can be used to send events notifications on stock prices of any companies and with this stock prices the timeboard can be created as well. Not just financial data, this code can be extended to be used for any news data.



