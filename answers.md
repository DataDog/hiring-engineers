## answers.md aka "My week with Datadog in my spare time"  

As an introduction, I am an Enterprise SE with extensive experience evangalizing Networking, Storage and Data Protection solutions.  This has been an intriguing (and occasionally humbling) exercise. In my career I have been a consumer of monitoring and analytics, not necessarily involved in the  dev/ops underpinnings that enable them other than providing feedback or feature requests.

As a testament to the the clarity of the effectiveness of the online Datadog documnentation installing and enabling the Datadog agent and specific stack integration was mostly a mattter of following the instructions, which even a non-dev ops SME can do :smiley:

I created the environment for this exercise by spinning up a Centos 7 VM in a virutal machine on a local hypervisor. I installed the Datadog agent for Centos and verified it was operational (see agent_status_before_MongoDB.txt in this branch). 

### Collecting Metrics    

I configured my default API key, listed at https://app.datadoghq.com/account/settings#api in **datadog.yaml**, the yaml file for the Datadog agent.  

    api_key: 584df05c35575f36e17d3543d00c341d  

Confirmed by "datadog-agent status"     

    API Key Status    
    ==============    
    https://6-2-0-app.agent.datadoghq.com,*************************c341d: API Key valid  
    
And added tags in the agent config file, and I also configured the agent yaml to report a specified host name 

    tags:
        - role:database:mongodb
    hostname: colby-exercise-machine.localdomain  




From agent status report:  

    Hostnames
    =========
      hostname: colby-exercise-machine.localdomain
      socket-fqdn: localhost
      socket-hostname: localhost.localdomain  

I next installed a single node MongoD and installed and configured Datadog integration for MongoDB
placing the statment in the mongo.yaml referencing the password for the datadog user I created in Mongo. 

    -   server: mongodb://datadog:kgVXlTnEFbTNSaKAdA7VYDf0@localhost:27017  
      
The collector section datadog-agent status report confirmed that the agent was successfully collecting data from the MongoDB instance

    mongo
    -----  
      Total Runs: 29444  
      Metrics: 113, Total Metrics: over 1M  
      Events: 0, Total Events: 0  
      Service Checks: 1, Total Service Checks: 29444  
      Average Execution Time : 21ms  

Per the exercise instructions I created a custom agent check to submit a metric (named "my_metric") that is a random value between 0 and 1000.  I decided to make the random "value" betweeen 0 and 1000 be an integer between 0 and 1000. 

**colbycheck.py**  

    import random
    from checks import AgentCheck
    class RandomCheck(AgentCheck):
     def check(self, instance):
      self.gauge('my_metric', random.randint(0,1000))  
 
 **colbycheck.yaml**
 
    self.init_config:

    instances:
      - name: colbyrandom
        min_collection_interval: 45  

I added a minimum collection configuration parameter for 45 seconds in the yaml file.  The exercise instructions specify changing my check's collection interval so it only submits the metric every 45 seconds. 

To be clear, however, the actual collection interval is a bit of a dance between the interval time per instance for this custom agent check and the interval time for Data Dog Agent collector (who's collection frequency is every 15-20 seconds dependent on how many integrations are enabled).  The result, per the documentation, is that this does not mean the metric is collected **every** 45 seconds but rather it **could** be collected **as often as** every 45 seconds.  


**Q: Can you change the collection interval without modifying the Python check file you created?**  
**A:** The collection interval is changed in the yaml file for the custom agent check, not in the python check file itself. 

To confirm the agent check and MongoDB integration are reporting to Datadog SAAS I reviewed the hostmap for exercise host and created a timeboard via the GUI   

![](hostmap.jpg)  &nbsp;&nbsp;&nbsp;&nbsp;
  
![](my_metric-random-integer-dashboard.jpg)  &nbsp;&nbsp;&nbsp;&nbsp;

### Visualizing Data  
Not having coded structured json before (in a prior role I sometimes had to review unstructured json generated by daily logging outpt of software product) I chose to "back in to" generating creating a Timeboard via structured json pushed up to the Datadog SAAS via the API by first pulling down the json of a working example timeboard and review its json. 

I first created a working example Timeboard, via the GUI, and used the API to issue a query to show a list of all my timeboards with  their ID numbers.  I then used the *timeboard_id* of the example timeboard to issue an API pull to download  the json definition of that timeboard, written to a file (*testtimeboard*).  I utilized *dogshell* to issue these API queries.  In this endeavor *dogshell* was my friend.  

    dog timeboard show_all
    810959	/api/v1/dash/810959	Michael's Visualization Reference Timeboard	created by mcolby@netzero.com
    809847	/api/v1/dash/809847	My_Metric	created by mcolby@netzero.com
    809827	/api/v1/dash/809827	Random Check	created by mcolby@netzero.com
  
    dog timeboard pull 810959 testtimeboard  


 Important to note that the GUI also provides a "point and click" means to view the json for any graph in a Timeboard.  The net result is multiple means to explore and analyze the json of a Timeboard for the json uninitiated.  

![](gui_showing_json.jpg)  &nbsp;&nbsp;&nbsp;&nbsp;

After I deciphered the json i used curl, via a shell script, to post the json for a new Timeboard.  


### Monitoring Data  
### Collecting APM Data  
### Final Question  
I don't know if this a creative use case,  but as a prior SE for a storage vendor for several years I had several Animation/Special Effects (SFX) Rendering customers.  One of which trusted me so strongly they would not let my employer replace me with another SE over the course of several years as sales reps transitioned in and out of the account.   

In full disclosure, another one of my ex-customers, Dreamworks Feature Animation, is a Datadog customer and I visit semi-annually with their long time operational and technology strategy architect to ruminate and chat about the state of the world and the state of storage and compute technologies. When I began discussions with folks at Data Dog I asked my friend if they use Datadog. I didn't ask how they use it, just for his thoughts on Datadog. His answer back was it is their "go to" for telemetry, monitoring and dashboards. When I noted what appears to be the flexibility and power of open sourced agents with native support for a large number of stacks and ecosystems with API integration extensability - with reporting delivered as a SAAS his reply was "exactly;  we're a happy customer."

I've always been fascinated by the similarity of animation/SFX rending workflows to an industrial factory manufacturing things (widgets, cars, toasters, etc).  The similarity is that in both employ a process pipeline of parts and raw materials come in on one side of the pipeline and some kind of finished product comes out at the other end.  In the case of animation/sfx the raw materials are millions/tens of millions/hundreds of millions (etc) of graphic elements, geometry and texture files, color tables, and lines of code for scheduling software, "render engine" software and various kinds of process software. On the other end the finished product is fully rendered animated feature movies and various images and rendered content used for marketing/promotion.  

manufacturing machines, human labor, robots, warehouses, labor, varioius methods of inventory management.
racks and racks of farms of "render machines," storage "farms" (usually some form of NAS), distriubted systems populated with cached data  









