## answers.md aka "My week with Datadog in my spare time"    
![Alt Text](https://m.popkey.co/818090/ObDJL.gif)

Thank you for the opportunity to complete this exercise. It has been a powerful learning experience and meaningful introduction to Datadog. 

I am an Enterprise SE with extensive experience evangalizing Networking, Storage and Data Protection solutions.  This has been an intriguing, learning, and occasionally humbling exercise. In my career I have been a consumer of monitoring and analytics, but not necessarily involved in the dev/ops underpinnings that enable them other than providing feedback or feature requests.  While working on creating a timeboard via the API I am sure I began dreaming of json in my sleep.

As a testament to the the clarity of the effectiveness of the online Datadog documnentation, installing and enabling the Datadog agent and specific stack integration was mostly a mattter of following the instructions; which even a non-dev ops SME can do :smiley:

I created the environment for this exercise by spinning up Centos 7 VMs on a local hypervisor. I installed the Datadog agent for Centos and verified it was operational (see agent_status_before_MongoDB.txt in this branch). 

### Collecting Metrics    

I configured my default API key, listed at https://app.datadoghq.com/account/settings#api in **datadog.yaml**, the yaml file for the Datadog agent.  *(because the first time I installed the agent I did it manually instead of using the easy one step install from the Agent install page)*.

    api_key: 584df05c35575f36e17d3543d00c341d  

Confirmed by "datadog-agent status"     

    API Key Status    
    ==============    
    https://6-2-0-app.agent.datadoghq.com,*************************c341d: API Key valid  
    
And added tags in the agent config file, and I also configured the agent yaml to report a specified host name 

    tags:
        - role:database:mongodb
    hostname: colby-exercise-machine.localdomain  

From agent status report:  

    Hostnames
    =========
      hostname: colby-exercise-machine.localdomain
      socket-fqdn: localhost
      socket-hostname: localhost.localdomain  

I next installed a single node MongoDB and installed and configured Datadog integration for MongoDB
placing the statment in the mongo.yaml referencing the password for the datadog user I created in Mongo. 

    -   server: mongodb://datadog:kgVXlTnEFbTNSaKAdA7VYDf0@localhost:27017  
      
The collector section datadog-agent status report confirmed that the agent was successfully collecting data from the MongoDB instance

    mongo
    -----  
      Total Runs: 29444  
      Metrics: 113, Total Metrics: over 1M  
      Events: 0, Total Events: 0  
      Service Checks: 1, Total Service Checks: 29444  
      Average Execution Time : 21ms  

Per the exercise instructions I created a custom agent check to submit a metric (named "my_metric") that is a random value between 0 and 1000.  I decided to make the random "value" betweeen 0 and 1000 be an ***integer*** between 0 and 1000. 

**colbycheck.py**  

    import random
    from checks import AgentCheck
    class RandomCheck(AgentCheck):
     def check(self, instance):
      self.gauge('my_metric', random.randint(0,1000))  
 
 **colbycheck.yaml**
 
    self.init_config:

    instances:
      - name: colbyrandom
        min_collection_interval: 45  

I added a minimum collection configuration parameter for 45 seconds in the yaml file.  The exercise instructions specify changing my check's collection interval so it only submits the metric every 45 seconds. 

To be clear, however, according to the documentation the actual collection interval is a bit of a dance between the interval time per instance for this custom agent check and the interval time for Data Dog Agent collector (who's collection frequency is every 15-20 seconds dependent on how many integrations are enabled).  The result, per the documentation, is that this does not mean the metric is collected **every** 45 seconds but rather it **could** be collected **as often as** every 45 seconds.  


**Bonus Question: Can you change the collection interval without modifying the Python check file you created?**  
The collection interval is changed in the yaml file for the custom agent check, not in the python check file itself. 

To confirm the agent check and MongoDB integration are sending metrics to Datadog I reviewed the hostmap for the exercise host and created a timeboard via the GUI   

![](hostmap.jpg)  &nbsp;&nbsp;&nbsp;&nbsp;
  
![](my_metric-random-integer-dashboard.jpg)  &nbsp;&nbsp;&nbsp;&nbsp;

### Visualizing Data  
Not having coded structured json before (in a prior role I sometimes had to review unstructured json generated by daily logging outpt of software product) I chose to "*backward engineer*" creating a Timeboard via structured json pushed up to the Datadog SAAS using the API by first pulling down the json of a working example timeboard and review its json. 

I first created a working example "reference" Timeboard, via the GUI and used the API to issue a query to show a list of all my timeboards with  their ID numbers.  I then used the *timeboard_id* of the example timeboard to issue an API pull to download  the json definition of that timeboard, written to a file (*testtimeboard*).  I utilized *dogshell* to issue these API queries.  In this endeavor *dogshell* was my friend.  Dogshell is handy tool to have. :smiley:

    dog timeboard show_all
    810959	/api/v1/dash/810959	Michael's Visualization Reference Timeboard	created by mcolby@netzero.com
    809847	/api/v1/dash/809847	My_Metric	created by mcolby@netzero.com
    809827	/api/v1/dash/809827	Random Check	created by mcolby@netzero.com
     
    dog timeboard pull 810959 testtimeboard  

Important to note, the GUI also provides a "point and click" means to view the json for any graph in a Timeboard.  The net result is multiple means to explore and analyze the json of a Timeboard for the json uninitiated.  

![](gui_showing_json.jpg)  &nbsp;&nbsp;&nbsp;&nbsp;

**json of Reference Timeboard**  


	{
	  "read_only": false,
	  "description": "created by Michael Colby in the GUI",
	  "created": "2018-05-14T22:45:44.976587+00:00",
	  "title": "Michael's Visualization Reference Timeboard",
	  "modified": "2018-05-22T05:07:50.684684+00:00",
	  "created_by": {
		"handle": "mcolby@netzero.com",
		"name": "Michael Colby",
		"access_role": "adm",
		"verified": true,
		"disabled": false,
		"is_admin": true,
		"role": "Datadog Recruiting Candidate",
		"email": "mcolby@netzero.com",
		"icon": "https://secure.gravatar.com/avatar/462e6a0ee1582516b52e571cf207bc36?s=48&d=retro"
	  },
	  "graphs": [
		{
		  "definition": {
			"viz": "timeseries",
			"requests": [
			  {
				"q": "avg:my_metric{host:colby-exercise-machine.localdomain}",
				"style": {
				  "width": "normal",
				  "palette": "dog_classic",
				  "type": "solid"
				},
				"type": "line",
				"conditional_formats": []
			  }
			],
			"autoscale": true
		  },
		  "title": "Avg of my_metric over host:colby-exercise-machine.localdomain"
		},
		{
		  "definition": {
			"viz": "timeseries",
			"status": "done",
			"requests": [
			  {
				"q": "avg:my_metric{*} by {role}.rollup(sum, 3600)",
				"aggregator": "avg",
				"style": {
				  "width": "normal",
				  "palette": "dog_classic",
				  "type": "solid"
				},
				"type": "area",
				"conditional_formats": []
			  }
			],
			"autoscale": true,
			"events": [
			  {
				"q": "my ",
				"tags_execution": "and"
			  }
			]
		  },
		  "title": "my_metric with 3600 second rollup"
		},
		{
		  "definition": {
			"viz": "timeseries",
			"status": "done",
			"requests": [
			  {
				"q": "anomalies(avg:mongodb.mem.resident{host:colby-exercise-machine.localdomain}, 'basic', 2)",
				"aggregator": "avg",
				"style": {
				  "width": "normal",
				  "palette": "dog_classic",
				  "type": "solid"
				},
				"type": "line",
				"conditional_formats": []
			  }
			],
			"autoscale": true
		  },
		  "title": "Avg of mongodb.mem.resident over host:colby-exercise-machine.localdomain"
		}
	  ],
	  "template_variables": [
		{
		  "default": "*",
		  "prefix": null,
		  "name": "var"
		}
	  ],
	  "id": 810959
&nbsp;
**Reference Version of visualizing data timeboard, created in the GUI**

![](visualization-reference-timeboard-1day.jpg)&nbsp;&nbsp;  

I reviewed the json of the Reference Timeboard and then used that as the basis for the json for the API created timeboard.  

I utilized an iterative process, starting with script to create a timeboard with just one of the 3 graphs.  Once that worked, I spun off another version of the script and added the json for creating the 2nd graph.   
So far so good. 

    ./createtimeboard2elementsworking.sh 

Successfully creating the Timeboard via API results in response confirming the json pushed up to *api.datadoghq.com*  

 ![](output2elementapijson.jpg)&nbsp;&nbsp;
&nbsp;
**API Created Dashboard with 2 graphs using my custom metric**

 ![](2elementapitimeboard.jpg)&nbsp;&nbsp;

It is at this point things got wonky. I can create a timeboard via the API that includes json for the first 2 graphs (that query *my_metric*) but with addition of json for the 3rd graph, which queries a MongoDB metric, *mongodb.mem.resident* it fails with a query parser error:  *Rule 'scope_expr' didn't match at ', 2)' (line 1, column 83)*.

    ./createtimeboard3elements.sh 
    {"errors": ["Error(s) found in query:\nError parsing query: \n unable to parse anomalies(avg:mongodb.mem.resident{host:colby-exercise- machine.localdomain}, basic, 2): Rule 'scope_expr' didn't match at ', 2)' (line 1, column 83)."]}  
    
&nbsp;

	api_key=584df05c35575f36e17d3543d00c341d
	app_key=4f8e48cb7882de1fced12344f6e42de7fad9ad49

	curl  -X POST -H "Content-type: application/json" \
	-d '{
		  "graphs" : [{
			  "title": "Avg of my_metric over host:colby-exercise-machine.localdomain",
			  "definition": {
				  "events": [],
				  "requests": [
					  {"q": "avg:my_metric{host:colby-exercise-machine.localdomain}"}
				  ]
			  },
			  "viz": "timeseries"
		  },
		  {
		  "title": "my_metric with 3600 second rollup",
			  "definition": {
				  "events": [],
				  "requests": [
					  {"q": "avg:my_metric{*} by {role}.rollup(sum, 3600)",
									"aggregator": "avg}"}
				  ]
			  },
			  "viz": "timeseries"
			  },
		  {
		  "title": "Avg of mongodb.mem.resident over host:colby-exercise-machine.localdomain",
			  "definition": {
				  "events": [],
				  "requests": [
					  {"q": "anomalies(avg:mongodb.mem.resident{host:colby-exercise-machine.localdomain}, 'basic', 2)"
				  ]
			  },
			  "viz": "timeseries"


		  }],
		  "title" : "Visualization Exercise Timeboard aka Fun with JSON",
		  "description" : "coding json for fun and entertainment",
		  "template_variables": [{
			  "name": "host1",
			  "prefix": "host",
			  "default": "host:my-host"
		  }],
		  "read_only": "True"
		}' \
	"https://api.datadoghq.com/api/v1/dash?api_key=${api_key}&application_key=${app_key}"

I tried in multiple iterations to create a query that for this metric (or any metric) with anomaly detection enabled, wthout success.  

At this point, however,  I have demonstrated successfully creating a timeboard via the API. For the purpose of expedience I will continue to debug and pursue resolution of rejection of this 3rd graph's query pushed up as json and from here demonstrate the requested changes  to the timeboard UI using my Reference version of the Timeboard created in the GUI

I set the timeboard's timeframe to 5 minutes, easy to do by grabbing along the graph and selecting the time range.  For this exericse I selected the most recent last 5 minutes, but I could have selected any 5 minute period to display. Because this is a *timeboard* versus a *screenboard* the selected time interval applies to all of the graphs in the timeboard simultaneously.  

After setting the timeboard's timeframe to the most recent past 5 minutes I took a snapshot of this graph and used @ notation in the comments to email it to my email address.  I probaby should have added some pithy commentary about graph in the comments.  Instead, I'll let my answer to this section's bonus question suffice for that pithiness.

 ![](snapshotmongodbmetricanomaly5min.jpg)&nbsp;&nbsp;
 

**Bonus question** 
I chose to graph *mongodb.mem.resident*, amount of memory currently used by the database process.  It appears that each time I populated MongoDB with additional documents the metric value increases.  The *basic* anaomoly detection algorithm appears to detecte each step change in Mongo memory usef as an anomoly. Not surprising since i intententionally didn't add documents in a consistent pattern, running the my data population script in sporadic intervals and pouplating varying amounts of additional documents each time.  During this interval i added 5,000 and 10,0000 1K documents.  


### Monitoring Data

Per the exercise specifications I created a new metric monitor for *my_metric* that alerts if the metric is above the following values over the past 5 minutes:   
* Warning Threshold of 500  
* Alerting Threshold of 800
* no data for this queried metric over the past 10 minutes  

I configured the monitor's messaging so an email is sent each time the monitor is triggered and utilized variables in the messaging  configuration to create different email messages based on what triggers the monitor:  Alert, Warning or No Data state.  I inlcluded in the message the value that caused the trigger, the measured average value what threshold it exceeded, and the host name and IP address of the source machine. 

A bit of anecdoatal observation:  I may have been suffering from *watched pot never boils syndrome*, but, as I was editing and testing the configuration of the alert messaging it seemed to me that although the alerting was working and was generating events that could be viewed on the events page in the Datadog GUI I was not receiving emails. In a bit of sanity checking I changed the warning theshold to *1* and the alert threshold to *2* to force a flow of alerts.  I also added 2 other email addresses to be notiifed.  I don't know if it was just impatience, or magic or *"things happen"* but after a bit of time alert emails started flowing to the 3 email addresses. After which I deconfigured 2 of the email destination from the messaging and reverted the threshold back to warning above 500 and alert above 800.  


![](define-threshold-alert.jpg)&nbsp;&nbsp;

    {{#is_alert}}  
    **Alert Threshold Exceeded**    
    {{value}} {{comparator}} {{threshold}}  
    on Host **{{host.name}}**   IP Address **{{host.ip}}**  
    {{/is_alert}}  
    
    {{#is_warning}}  
    **Warning Threshold Exceeded**    
    {{value}} {{comparator}} {{warn_threshold}}  
    on Host **{{host.name}}**   IP Address **{{host.ip}}**  
    {{/is_warning}}  
    
    {{#is_no_data}}  
    **Warning Warning Warning - No Data Received 10 Minutes - Please Check!**  
    From Host **{{host.name}}**  IP Address **{{host.ip}}**  
    {{/is_no_data}}  
    
    @mcolby999@gmail.com

**Email Alert for exceeded warning threshold**  
![](warning-threshold-exceeded-757.jpg)&nbsp;&nbsp;

**Email Alert for exceeded alert threshold**  
![](alert-threshold-exceeded-862.jpg)&nbsp;&nbsp;&nbsp;&nbsp;  

**Bonus Question** 
I created 2 recurring scheduled downtimes.  One daily for the 14 hours of 7pm to 9am and the other for all day Saturday and Sunday.  Note, my intepretation of *all day* is 12am to the following 12am. 

***I just realized there's an alternate way to configure a schedule of 7Pm to 9am.  Instead of configuring daily 7pm for 14 hours, instead schedule weekly M,T,W,T,F 7PM for 14 hours.  I've added a screen shot of this example below.***  


**Configuration of *Daily* Downtime**  
![](downtimedaily.jpg)&nbsp;&nbsp;  

***Altnernate way to schedule Daily Downtime***  
![](alternatedailydowntimeschedule.jpg)&nbsp;&nbsp;

**Configuration of *All Day Saturday and Sunday* Downtime**  
![](downtimeweekend.jpg)&nbsp;&nbsp;  

***Daily*** **Downtime email creation notification**  
![](downtimeweekdayemailnotificationv2.jpg)&nbsp;&nbsp;  


***Weekend*** **Downtime email creation notification**  
![](downtimeweekendemailnotification.jpg)&nbsp;&nbsp;  


***Daily*** **Downtime email notification**  
![](scheduled7pm.jpg)&nbsp;&nbsp;  
 

### Collecting APM Data  
Although the the tracing APM setup documentation (https://docs.datadoghq.com/tracing/setup/) states  "The APM Agent is enabled by default on Linux," that appears to be untrue. On the Centos 7 machines on which I installed the Datadog agent the APM *enabled: true* config option was commented out, as were all the other options, including the *receiver port* option.  

I uncommmented *apm_config*, *enabled: true* and *receiver_port: 8126*

	# apm_config:
	#   Whether or not the APM Agent should run
	#   enabled: true
	#   The environment tag that Traces should be tagged with
	#   Will inherit from "env" tag if none is applied here
	#   env: none
	#   The port that the Receiver should listen on
	#   receiver_port: 8126
	#   Whether the Trace Agent should listen for non local traffic
	#   Only enable if Traces are being sent to this Agent from another host/container
	#   apm_non_local_traffic: false
	#   Extra global sample rate to apply on all the traces
	#   This sample rate is combined to the sample rate from the sampler logic, still promoting interesting traces
	#   From 1 (no extra rate) to 0 (don't sample at all)
	#   extra_sample_rate: 1.0
	#   Maximum number of traces per second to sample.
	#   The limit is applied over an average over a few minutes ; much bigger spikes are possible.
	#   Set to 0 to disable the limit.
	#   max_traces_per_second: 10
	#   A blacklist of regular expressions can be provided to disable certain traces based on their resource name
	#   all entries must be surrounded by double quotes and separated by commas
	#   Example: ["(GET|POST) /healthcheck", "GET /V1"]
	#   ignore_resources: []

I installed *dd-trace* via *PIP*,  *flask*, and virtualenv.  There's probably a good chance I don't need to isolate flask in a virtual environment since exercise is hosted on a relatively minimal Centos machine that is used for nothing elese. I also installed *blinker* after a debug warning on an initial test indicated blinker had to be installed:  

    DEBUG:ddtrace.contrib.flask.middleware:please install blinker to use flask signals. http://flask.pocoo.org/docs/0.11/signals/
&nbsp;  

All that said, the app errors out with a *socket.error* *address already in use*.  This appears to be a common issue when running flask apps.  I'm not an experience application developer or experienced with flask. I'm going to continue debugging but dont want to hold up submitting the rest of this exerccise.  I'm issuing a pull request of this repository with my answers.md while I continue to work on sorting this out. 

	(venv) [root@datadog-testing ddtraceproject]# ddtrace-run python apmtest.py
	 * Serving Flask app "apmtest" (lazy loading)
	 * Environment: production
	   WARNING: Do not use the development server in a production environment.
	   Use a production WSGI server instead.
	 * Debug mode: off
	Traceback (most recent call last):
	  File "apmtest.py", line 28, in <module>
		app.run()
	  File "/ddtraceproject/venv/lib/python2.7/site-packages/flask/app.py", line 943, in run
		run_simple(host, port, self, **options)
	  File "/ddtraceproject/venv/lib/python2.7/site-packages/werkzeug/serving.py", line 814, in run_simple
		inner()
	  File "/ddtraceproject/venv/lib/python2.7/site-packages/werkzeug/serving.py", line 774, in inner
		fd=fd)
	  File "/ddtraceproject/venv/lib/python2.7/site-packages/werkzeug/serving.py", line 660, in make_server
		passthrough_errors, ssl_context, fd=fd)
	  File "/ddtraceproject/venv/lib/python2.7/site-packages/werkzeug/serving.py", line 577, in __init__
		self.address_family), handler)
	  File "/usr/lib64/python2.7/SocketServer.py", line 419, in __init__
		self.server_bind()
	  File "/usr/lib64/python2.7/BaseHTTPServer.py", line 108, in server_bind
		SocketServer.TCPServer.server_bind(self)
	  File "/usr/lib64/python2.7/SocketServer.py", line 430, in server_bind
		self.socket.bind(self.server_address)
	  File "/usr/lib64/python2.7/socket.py", line 224, in meth
		return getattr(self._sock,name)(*args)
         socket.error: [Errno 98] Address already in use

**UPDATE**  
I am not a facile programmer, but I do have a process for identifying symptoms and determining root cause(s).  Doing some homework on flask indicated that hen the app is invoked with app.run() it by default uses TCP port 5000.  Checking what TCP ports processes are using, by issuing  *netstat* with  *-p -l -n -tcp* shows that the Datadog agent binds and listens to TCP ports 5000 and 5001, which would conflict with my flask app if it is also trying to use TCP port 5000.  

    Active Internet connections (only servers)
    Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
    tcp        0      0 127.0.0.1:8126          0.0.0.0:*               LISTEN      1025/trace-agent    
    tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN      1024/agent          
    tcp        0      0 127.0.0.1:5001          0.0.0.0:*               LISTEN      1024/agent          
    tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1023/sshd           
    tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      2184/master   
    
Further homework on flask indicates that it is possible to explicitly set the host ip address and port number used by app.run()
  
So I modified my flask app to set the TCP port number to 4999 (a bit of humor: long ago automobile body shop pitchman Earl Scheib used to promise to "paint any car for $49.99).  The result is the that my flask app (apmtest.py) appears to be running successfully invoked with dd-trace.


	(venv) [root@datadog-testing ddtraceproject]# vi apmtest.py
	(venv) [root@datadog-testing ddtraceproject]# ddtrace-run python apmtest.py
	 * Serving Flask app "apmtest" (lazy loading)
	 * Environment: production
	   WARNING: Do not use the development server in a production environment.
	   Use a production WSGI server instead.
	 * Debug mode: off
	2018-05-22 23:10:56,231 - werkzeug - INFO -  * Running on http://127.0.0.1:4999/ (Press CTRL+C to quit)

&nbsp;  

	from flask import Flask
	import logging
	import sys

	# Have flask use stdout as the logger
	main_logger = logging.getLogger()
	main_logger.setLevel(logging.DEBUG)
	c = logging.StreamHandler(sys.stdout)
	formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
	c.setFormatter(formatter)
	main_logger.addHandler(c)

	app = Flask(__name__)

	@app.route('/')
	def api_entry():
		return 'Entrypoint to the Application'

	@app.route('/api/apm')
	def apm_endpoint():
		return 'Getting APM Started'

	@app.route('/api/trace')
	def trace_endpoint():
		return 'Posting Traces'

	if __name__ == '__main__':
		app.run(host=None, port=4999)

I am still not out of the woods yet.  No collectd APM data has shown up in the Datadog GUI yet.  I'm going to do some more debugging.

More to come......









**Bonus Question**
The difference between a service and a resource is that a ***service*** is a set of processes that together deliver a feature set.  
A ***resource*** is a specific query to a service.  

This is explained in Datadog help documenation for APM at:  
https://help.datadoghq.com/hc/en-us/articles/115000702546-What-is-the-Difference-Between-Type-Service-Resource-and-Name-
and in the *Getting Started* APM documentation: 
https://docs.datadoghq.com/tracing/visualization/





### Final Question  
I don't know if this a creative use case,  but as an SE for a storage vendor for several years I had several Animation/Special Effects (SFX) Rendering customers.  The staff at one of them trusted me so strongly they would not let my employer replace me with another SE over the course of several years as sales reps transitioned in and out of the account.   

In full disclosure, another one of my ex-customers, Dreamworks Feature Animation, is a Datadog customer and I visit semi-annually with their long time operations and technology strategy architect. When I began discussions at Data Dog I asked my friend if they use Datadog. I didn't ask how they use it, just for his thoughts on Datadog. His answer back was it is their "go to" for telemetry, monitoring and dashboards. When I noted the flexibility and power of open sourced agents with native support for a large number of stacks and ecosystems with API integration extensability - with reporting delivered as a SAAS his reply was "***exactly;  we're a happy customer***."

I've always been fascinated by the similarity of animation/SFX rending workflows to an industrial factory manufacturing things (widgets, cars, toasters, etc).  The similarity is that both employ a *process* pipeline in components and raw materials come in on one side of the pipeline and some kind of finished product comes out at the other end.  In the case of animation/sfx the raw materials are millions/tens of millions/hundreds of millions (etc) of graphic elements, geometry and texture files, color tables, and lines of code for scheduling software, "render engine" software and various kinds of process software. On the other end the finished product is fully rendered animated feature movies and various images and rendered content used for marketing/promotion.  

A manufacturing plant utilizes machines, human labor, manufacturing robots, pysical warehouses, and various methods of inventory management of the contents of those warehouses. 

In comparison the owners of a digital animation pipeline employ:
* racks of "render" "*compute farms*"  
* *storage farms* (usually some form of NAS)
* A layer of network data caches populated with frequently accessed data cached from the storage farms.
* human labor in the form of visual artists creating raw graphic elements on high performance graphics workstations
* some kind of system/process for managing metadata reference for all the digital "objects" and "elements" used in rendering image frames - matching up objects and elements with their storage paths/locations.  
* Inventory of commmerical and local proprietary code used to perform data rendering, and to manage scheduling of processes of data rendering and data transformation.   

A successful animation rendering enteprise usually has a pipeline of 3-5 separate movies with closely spaced releases dates. To meet these often tightly scheduled timelines the compute and storage resources operate 24x7. The goal for a render farm can be 95% utilization 24x7.  Anything that slows down, or stops rendering is a potential bottleneck that threatens delivering these multiyear projects on time.  Like a slowdown or a line stop on an industrial factory floor.  These issues and events can include   
* exceeding avaialble compute resources
* network i/o bottlnecks
* storage i/o bottlenecks*
* machine failures and storage hardware failures. 

Even small increases in latency ***anywhere*** in the process pipeline can result in days or weeks of negative schedule impact.  Pipeline success is measured in the amount of rendered content completed over time (hourly, daily, weekly).  

There are various places in the process that can be chokepoints or bottlenecks or critical failure points.  These need to measured and monitored. Animation/SFS rendering pipeline success is measured in SLA delivery of amount of rendered content completed over time (hourly, daily, weekly) resulting in delivered final content at scheduled completion time for scheduled release dates.  Missing  SLAs and as a result missing deadlines for release is **not** an option. 

I envision an instrumentation and monitoring regime that involves measuring *up* and measuring *down*.  Measuring and monitoring *up*  Data Center environmentals, operational status and heath of physical machines, Network I/O, storage I/O (aggregate and per storage system), corequest latency for sampled data objects, etc.  And measuring "down* from measuring delivery of SLAs, e.g. Frames rendered, Render Farm productivity, etc.  

As an example, from my personal exeperience with another Animation/SFX render customer, they complained to me that after  expanding their storage farm with several of our latest generation NAS storage systems they were experiencing what they believed were a higher than expected rate of disk failures.  If their operation was well instrumented using Data Dog they would have been able to focus on Problems instead of symptoms.  They would have known, was their rendering pipeline impacted and to what extent.  Instead all they knew was the *perceived* symptom. 

In my experience this particular customer's staff frequently complained about *perceived* symptoms that inevitaby did not correspond to empirical data.  Not unrelated to my experience of wating to receive alert emails for this exercise.  Nevertheless, as an SE who always  advocates for customers, I offered to conduct empirical analysis to confirm or disprove the conclusions of their perception.  I created a spreadsheet based comparison of number of disks failed (and subsequent i/o reducing RAID rebuids) per month over a period of several years compared to a  predicted failure rate based on published MTBF times x the number of drives in currently deployed systems each month.  When I grapheded the data it exposed that they were, in fact, experiencing a higher than predicted  disk failure rates on our newest systems most recently sold to them. It turned out that I had stumbled on a known issue that our support organization was already aware of, but which hadn't yet been shared with the field and impacted custmers:  a specific generation of a specific vendors' drives in our newest systems were experiencing **high** "infant mortality" rates. 

While my spreadsheet based analysis validated that their perception was accurate, the process of creating that analysis was slow, tedious, and subject to human/typing data input error(s).  Using Datadog  the whole pipeline and the systems used to enable it could have been instrumented.  In this case the specific analyis could have been pre-automated for easy visual review, including anamoly detection.   A timeboard of disk failures graphed against a calculated predicted failure rates automatically based on metrics of # of disks in operation would have exposed the issue right away.  Additional instrumentation could have triggered alerts that theshholds of operational SLA metrics (render rate, render latency, a disk failure threshold per NAS system) were exceeded. Or, based on detection of this issue a "drill up" of metrics would have highlighted the specific measured impact this issue creating on SLAs and operational metrics.   Reviewing Datadog documentation on using Datadog to monitor **SNMP** enabled devices this appears to be quite feasible to implement for "black box" appliance systems like enterprise NAS solutions (e.g. Netapp FAS, EMC Unity, EMC Isilon, Quumulo, Cohesity, etc)











