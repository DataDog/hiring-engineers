# 5/23/2018 - Matt Lee

### For all referenced screenshots, these should be uploaded to the repo. If not, please refer to: https://drive.google.com/drive/folders/1Zqz5rcJkE-1aCFksUctqIJTX7vWt2mWT?usp=sharing 

## Collecting Metrics
  * Add tags in the Agent config file and show us a screenshot of your host and its tags on the Host Map page in Datadog.
      * **To complete this, I added the following tags directly in the config file on my host: "region:us-west-2, env:aws, role:ubuntu1". This task in particular was straightforward and customers should find this process both easy and powerful in its flexibility. This allows metrics later to be sliced & diced by the different dimensions when these tags are at the lowest host-level.**
      * **This was completed directly adding the files as seen below, and validating that the tags were coming in through in the out of the box Infrastructure Host Map visualization. Given that Datadog's responsiveness is so quick, this allows for any customer to be able to validate their tasks to ensure their tagging is adhereing to their expectations.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/1-tags1.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/1-tags2.PNG)
  * Install a database on your machine (MongoDB, MySQL, or PostgreSQL) and then install the respective Datadog integration for that database.
      * **For this particular task, I installed PostgreSQL on my host, and its respective Datadog integration. The entire process was very straightforward, with following both online documentation as well as the in-product guide for the PostgreSQL integration instructions. Once I got the integration working, I then executed several create table commands along with inserts to validate that these metrics were being correctly monitored and tracked by Datadog.**
      * **For any customer going through the process of adding integrations to their hosts, this was eye-opening about how painless the end-to-end process was like. The appropriate user settings were clearly outlined in the integrations windows, and I would imagine that most customers will delighted to have everything in one place to complete their integration.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/2-postgres1.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/2-postgres2.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/2-postgres3.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/2-postgres5.PNG)
      * **As I also deployed my host on AWS, I decided to complete the AWS integration, which was straightforward in its instructions as well. I hit a few snags with setting up the IAM roles appropriately, but had that figured out after a few attempts with setting up the right policies and user roles. You can validate that the AWS and PostgreSQL integrations were set up in the following image.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/2-postgres4.PNG) 
      
  * Create a custom Agent check that submits a metric named my_metric with a random value between 0 and 1000.
      * **I created a custom agent check in Python that creates a random value between 0 & 1000 named "my_metric". This was again straightforward from reading through your documentation of how to create a custom agent check, with few modifiers to include the random value metric. Validating this was also easy by checking in the Web UI that the my_metric metric was being reported correctly along with the random values populating as expected!** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/3-customMetric1.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/3-customMetric2.PNG)
      * **The ability to create custom agent checks quickly and using Python is phenomenal. It's easily accessible to majority of DevOps along with any technical program managers that may want to include any custom metric reporting on their hosts.**
  * Change your check's collection interval so that it only submits the metric once every 45 seconds.
      * **For this particular task, I added a wait timer in the agent check directly in the python code. I used the 'time.sleep()' function to pause the code for 45 seconds before it submits the metric. Given the amount of flexibility offered to customers and myself to modify these custom agent checks, this was a pleasant surprise to see it work as I expected.** 
      * **In the following screenshots, you can see how the intervals are being correctly tracked as the random metrics are now submitted in 45 second increments.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/4-customMetric1.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/4-customMetric2.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/4-customMetric3.PNG)
  * Bonus Question Can you change the collection interval without modifying the Python check file you created?
      * **For this, I refered to some additional documentation that showed how the agent check's YAML files contain multiple different parameters to modify its behavior. Within there, I included the notation of the "min_collection_interval" parameter and thus am able to keep the original python code and remove the 'time.sleep()' function and keep it submitting to 45 second submission cycles.**
      * **This is great for customers to have a variety of options to not only configure their custom agent checks directly, but given configuration settings that can be replicated in a repeatable fashion.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/5bonus-customMetric1.PNG) ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/5bonus-customMetric2.PNG) ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/5bonus-customMetric3.PNG) 
      
## Visualizing Data:
* Utilize the Datadog API to create a Timeboard that contains:
  * Your custom metric scoped over your host.
  * Any metric from the Integration on your Database with the anomaly function applied.
  * Your custom metric with the rollup function applied to sum up all the points for the past hour into one bucket
      * **For this particular task, once I got ramped up on the Datadog API documentation, I created the following python call that would execute the creation of the Timeboard with the following metrics. Getting the appropriate structure was a little tricky, but it was straightforward once I saw some examples online on how these visuals had the proper query structure. Learning about where the anomaly function was defined and described was also a little tricky, but I got to find the proper documentation on it along with testing it out within the UI before incorporating it within the API call to ensure it would create the right chart.**
      * **As can be seen below in the script, along with the corresponding timeboard that was created, this Timeboard creation request was completed without much issues. And if there was ever any issue, it's nice for customers to have the Datadog UI to validate it very quickly.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/Visualizing_Data_2.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/Visualizing_Data_1.PNG)

* Please be sure, when submitting your hiring challenge, to include the script that you've used to create this Timeboard.
  * **Absolutely. Please find the script used to create the Timeboard @ [Git Link](https://github.com/m4ttlee/hiring-engineers/blob/master/createTimeboard.py) OR [Google Drive Link](https://drive.google.com/open?id=1ug8WelXZw4IcX0VFr09_Ejx5oJDsIbiZ)**

* Once this is created, access the Dashboard from your Dashboard List in the UI:
  * Set the Timeboard's timeframe to the past 5 minutes
      * **From accessing the newly created Dashboard, to adjust the Tiemframe to the past 5 minutes, I selected on a chart the past 5 minutes that were relevant to me, and noted that the timeframe of the Timeboard had been now updated to reflect my desired timeframe. This seem very straightforward to me as I would expect most customers to behave that way beyond the standard timeframe selections in the dropdown.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/Visualizing_Data_past5Mins.PNG) 
  * Take a snapshot of this graph and use the @ notation to send it to yourself.
      * ** To take a snapshot, I found this easy to find within the UI itself. I simply clicked on the camera icon, and a new window popped up and included the '@' notation to send it to myself. I'd venture that customers love this feature as they can not only make references to themselves easily, but can send charts of interest to their team members with a few simple clicks. This makes collaboration within the product very accessible and easy to do.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/Visualizing_Data_3.PNG)
  * Bonus Question: What is the Anomaly graph displaying?
      * **The anomaly graph is displaying a visualwhen a metric is behaving differently than it has in the past, taking into account trends including seasonal day-of-week and time-of-day patterns. In this case, it's displaying an anomaly of the database create table counts and noting it to be an abnormal spike. This is immensely valuable for customers as the identification and presentation of metrics can be challenging with strong trends and recurring patterns that are hard or impossible to monitor with threshold-based alerting.**

## Monitoring Data
Since you’ve already caught your test metric going above 800 once, you don’t want to have to continually watch this dashboard to be alerted when it goes above 800 again. So let’s make life easier by creating a monitor.

* Create a new Metric Monitor that watches the average of your custom metric (my_metric) and will alert if it’s above the following values over the past 5 minutes:
  * Warning threshold of 500
  * Alerting threshold of 800
  * And also ensure that it will notify you if there is No Data for this query over the past 10m.
  * Send you an email whenever the monitor triggers.
  * Create different messages based on whether the monitor is in an Alert, Warning, or No Data state.
  * Include the metric value that caused the monitor to trigger and host ip when the Monitor triggers an Alert state.
      * **Creating the monitor was very straightforward with the Wizard walking through most of the steps that I needed to fill out as outlined above. The initial challenge was finding the right syntax for creating the custom message dependent on the monitor states. Once I found the appropriate language in the documentation, the rest of the monitoring configuration was completed. I would venture that many customers would find the documentation library super useful in these types of situations for creating all of their respective tasks!** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/MonitoringData_1.PNG)
      
* When this monitor sends you an email notification, take a screenshot of the email that it sends you.
  * **With the outlined parameters, it didn't take long for the monitor to hit the warning threshold at which point I received an email from Datadog with my appropriate message. This is super useful for customers to not only be able to validate that their monitors are working correctly, but see the types of detail that can be captured in the message. Such that it includes the value of the metric along with respective detail to the trigger. This is absolutely compelling for most customers to be able to not only intelligently reactive, but set up the right proactive steps in warning functions.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/MonitoringData_0.PNG)   
* Bonus Question: Since this monitor is going to alert pretty often, you don’t want to be alerted when you are out of the office. Set up two scheduled downtimes for this monitor:
  * One that silences it from 7pm to 9am daily on M-F,
  * And one that silences it all day on Sat-Sun.
  * Make sure that your email is notified when you schedule the downtime and take a screenshot of that notification.
      * **Upon a little digging, this feature is readily accessible within the UI as 'Manage Downtime'. The definitions of configuring a new downtime for my newly created monitor was also very straightforward. I played around a bit to see if there was a way to state two different times for different days, in the above case where I had to define two situations: weekdays and weekends. I was not able to find a downtime manager that could apply to the same monitor with multiple conditions, so I simply created two downtime managers, one to manage the weekdays, and one for the weekends. This part could be potentially enhanced as I would imagine customers may want to apply downtime at the monitor itself as a whole, instead of creating separate downtimes depending on the rules they have.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/MonitoringData_2.PNG) ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/MonitoringData_3.PNG) ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/MonitoringData_4.PNG) 
    
## Collecting APM Data:
* Given the following Flask app (or any Python/Ruby/Go app of your choice) instrument this using Datadog’s APM solution:
  * **I completed this by setting up my python app with APM, and following the instructions for setting this up. With installing ddtrace on my host, the rest was very straightforward. Within a few minutes of initializing my app with the ddtrace call, I started seeing traces come in through to the Datadog UI. I would venture that customers must be delighted about how easy this is to configure with Python as it removes quite a bit of pain if they had to configure custom code in each of their apps. My python app (linked below) is using Twitter's API to pull in relevant targeted tweets (in my case: NFL, fantasy football, Seattle, Seahawks), creating a database, and writing those tweets to the database.**. ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/APM_2.PNG)![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/APM_3.PNG)

* Bonus Question: What is the difference between a Service and a Resource?
  * **A service is the set of the processes that are defined by the user when instrumenting with Datadog. In my app's case, the primary service is the sqlite service that is being tracked in its usage. A resource is the particular query to the service. In my app's case, these are the SQL queries as seen in the following image.** ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/APM_4.PNG)

* Provide a link and a screenshot of a Dashboard with both APM and Infrastructure Metrics.
   * **After I was tracking the performance of my python app, I then created the following dashboard called "Matt's Dashboard" that has both Infrastructure and APM metrics. I found this relatively straightforward, with pulling the APM metrics from the 'APM' dashboards that were created automatically. I then published and pinned them to my Dashboard.** [The current link to the dashboard](https://app.datadoghq.com/dash/817934/matts-dashboard?live=true&page=0&is_auto=false&from_ts=1527033243068&to_ts=1527036843068&tile_size=l&fullscreen=false). ![Image](https://github.com/m4ttlee/hiring-engineers/blob/master/APM_1.PNG)
   
* Please include your fully instrumented app in your submission, as well.
   * **Sure. Please find the python app called 'scraper.py' in the repo, as linked** [here](https://github.com/m4ttlee/hiring-engineers/blob/master/scraper.py)  
   
## Final Question:
* Datadog has been used in a lot of creative ways in the past. We’ve written some blog posts about using Datadog to monitor the NYC Subway System, Pokemon Go, and even office restroom availability! Is there anything creative you would use Datadog for?
   * **I'd love to see how Datadog could monitor the spike of web traffic and its impact on site functionality (page loads, commenting, etc) on social sites like Reddit. I've seen this traffic take down a variety of services on the site itself, from popular AMA's, SuperBowl games, and premiere movie releases. It'd be great for them to be proactive, prevent outages and deprecation of core services. I'd love to understand their infrastructure strategy and understand what about the loads cause certain subreddits to load quicker than others, and for certain tasks to be deprecated in functionality: likes, posts, commenting, etc.**
