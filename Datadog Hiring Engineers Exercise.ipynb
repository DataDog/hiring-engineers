{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Docker Logo](https://www.vectorlogo.zone/logos/docker/docker-card.png)\n",
    "This testing was done on a Paperspace VM running Ubuntu 18.04. I had started with Docker on Windows which appeared adequate at first, and then caused me trouble. I learned that composed Docker stacks on a local swarm running on Windows doesn't network the same as Linux environments.\n",
    "\n",
    "![Ubuntu on Windows](https://www.windowslatest.com/wp-content/uploads/2017/07/Ubuntu-on-Windows-10-696x348.jpg)\n",
    "I then tried the \"Ubuntu\" available in the Windows Store. Everything worked until I discovered there is no /proc filesystem since it's not even running the Linux kernel. Not having a /proc filesystem leads to a strange lack of information when using typical Linux tools. Too much wasted time on that system!\n",
    "\n",
    "![Paperspace Logo](https://odsc.com/wp-content/uploads/2018/01/paperspace-logo-300x168.jpeg)\n",
    "Frustrated, I installed both Ubuntu locally in a Hyper-V VM and spun up a Paperspace instace. Paperspace encountered errors during provisioning, so I worked on the slow local install for a while. Once Paperspace errors resolved, their VM was significantly faster than my local machine so I stuck that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I developed a simple api with 6 resources: 3 evil, 3 nice. Since my app is based on Flask, I used the third-party trace-middleware as desribed in the [docs](https://docs.datadoghq.com/tracing/setup/python/#example-simple-tracing)\n",
    "\n",
    "### source code in ./api/\n",
    "\n",
    "I developed methods to post comments and events.\n",
    "\n",
    "![event posted](images\\event.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I installed the datadog agent using the preferred Ubuntu install script. I noticed that the provided script is prepopulated with necessary keys and that made installation a breeze.\n",
    "\n",
    "![agent exiting](images\\agent_exit.PNG)\n",
    "Something needs fixing, the agent won't stay running. Looks like I need to enable  the agent.\n",
    "\n",
    "I adjusted the datadog.yaml file to enable the agent and APM and added tags while I was there.\n",
    "![agent exiting](images\\epa.PNG)\n",
    "\n",
    "![agent exiting](images\\tic.PNG)\n",
    "\n",
    "Then I verified that the agent was listening on the expected port:\n",
    "![verified port](images\\confirm_8126.PNG)\n",
    "\n",
    "*note: 404 is the expected response.*\n",
    "\n",
    "And data is coming in!\n",
    "\n",
    "![new data](images\\errors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Check and Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I placed the following code in **/etc/datadog-agent/checks.d/** as\n",
    "\n",
    "**my_metric.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checks import AgentCheck\n",
    "import random\n",
    "class my_metric(AgentCheck):\n",
    "    def check(self, instance):\n",
    "        val = random.uniform(0,1000)\n",
    "        self.gauge('my_metric ', val, tags=[u'ddhee',u'maint:tmayse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I placed the following configuration in **/etc/datadog-agent/conf.d/my_metric/** as my_metric.yaml and while I was there, I skipped ahead and just configured the check to run **not more often than** every 45 seconds. \n",
    "\n",
    "**my_metric.yaml:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_config:\n",
    "\n",
    "instances:\n",
    "    - host: \"psh8wbmgg\"\n",
    "      min_collection_interval: 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the config **sudo service datadog-agent status**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agent config](images\\checks.PNG)\n",
    "\n",
    "*pleased to find my_check listed at the top*\n",
    "\n",
    "Timing confirmed in agent logs:\n",
    "![agent config](images\\45.PNG)\n",
    "\n",
    "And then, as configured, the check ran:\n",
    "![agent config](images\\check_ran.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags on infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As show earlier, configured tags:\n",
    "![tags shown in config file](images\\tic.PNG)\n",
    "\n",
    "![host tags in UI](images\\host_tags.PNG)\n",
    "\n",
    "Bonus Question Can you change the collection interval without modifying the Python check file you created? **see config & logs above**\n",
    "\n",
    "*in addition to the tags I configured, an automatic tag was added identifying the host*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I installed Postgresql as it's generally trouble-free. One of my endpoints (/tfs) stimulates the DB.\n",
    "\n",
    "The agent is configured by adding postgres.yaml to /etc/datadog-agent/conf.d/postgres.d/postgres.yaml\n",
    "\n",
    "**postgres.yaml:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "init_config:\n",
    "\n",
    "instances:\n",
    "  - host: localhost\n",
    "    password: CG4W1mPaET70QWl2TrjYeAlN\n",
    "    port: 5432\n",
    "    tags:\n",
    "      - ddhee\n",
    "      - \"role:db\"\n",
    "      - \"description:DDHEE db\"\n",
    "    username: datadog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Database Dashboard](images\\db_dashboard.PNG)\n",
    "\n",
    "[Link to board](https://app.datadoghq.com/screen/integration/235/postgres---overview?page=0&is_auto=false&from_ts=1532551260000&to_ts=1532554860000&live=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeboard created by API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed 4 tries to get this right. I copied the graph definitions from the UI dasboard's JSON output. I found that the anomolies method requires more arguments in a Timeboard. Once I fixed that, my script worked as-expected.\n",
    "\n",
    "**Tmeboard Script:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "curl  -X POST -H \"Content-type: application/json\" \\\n",
    "-d @graphs.ddhee.json \"https://api.datadoghq.com/api/v1/dash?api_key=e1dbdaceaf7516f90ef9e2ad5546072e&application_key=25b8d433ca6e9ca99c1ee791e8ece8c67e6a0ec3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dashboard created by API](images\\Dashboard.png)\n",
    "\n",
    "\n",
    "[dashboard link](https://app.datadoghq.com/dash/871365/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeboard with five minute window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took me a while to figure out. It wasn't immediately obvious to me that a Screenboard fit the bill. When I came upon **[How to Transform a Timeboard to a Screenboard or vice versa ?](https://docs.datadoghq.com/graphing/faq/how-to-transform-a-timeboard-to-a-screenboard-or-vice-versa/)** which links to [this script](https://github.com/DataDog/Miscellany/blob/master/dashconverter.py) Once my timeboard was converted to a Screenboard, I was able to set the time to each chart to five minutes.\n",
    "\n",
    "**here's the command line util I ran**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python dash --api-key e1dbdaceaf7516f90ef9e2ad5546072e --app-key 25b8d433ca6e9ca99c1ee791e8ece8c67e6a0ec3 --titl\n",
    "e \"From Timeboard\" 871365"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenboard Converted by scipt](images\\screenboard.png)\n",
    "\n",
    "[screenboard link](http://app.datadoghq.com/screen/393468)\n",
    "\n",
    "\n",
    "**Bonus Question: What is the Anomaly graph displaying?**\n",
    "\n",
    "It reads \"Not enough historical data for this algorithm.\"\n",
    "\n",
    "*note: I didn't explore other algorithms' data requirements.*\n",
    "\n",
    "I didn't figure out how to send a screenshot, so I sent myself a URL:\n",
    "![at mention for new screenboard](images\\atmention.png)\n",
    "\n",
    "The email I got as a result looked like this:\n",
    "![email from at mention](images\\atmentionemail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image has all the info necessary to verify notifications: Here's what I did:\n",
    "\n",
    "![monitor setup](images\\monitorpage.png)\n",
    "\n",
    "This is very noisy, of course. I'm glad configuring downtimes was part of the exercise.\n",
    "![downtimes scheduled](images\\downtime.png)\n",
    "\n",
    "https://app.datadoghq.com/monitors/5645900\n",
    "\n",
    "**note: For the weekend downtime, I was frustrated by an error that was telling me my start time was too early any time around midnight. I tried from 11:59PM Fri for two days, but that's clearly not right as it didn't have the desired effect. It only just now ocurred to me that the times are UTC. The email was clear about that, but I didn't read it. A better error message might say \"that time is in the past\", but ulitmately, this was my bad.**\n",
    "\n",
    "![downtime](images\\schedtime.png)\n",
    "\n",
    "In addition to an email, I captured this event in the event stream:\n",
    "\n",
    "![event in stream](images\\notif.png)\n",
    "\n",
    "I also enable suggested monitors for my API and was alerted\n",
    "![hammer](images\\hammer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://app.datadoghq.com/dash/871420\n",
    "    \n",
    "Included in this dashboard are process metrics, my_metric chaos, NTP offset, and average Apdex by resource.\n",
    "\n",
    "![App & Host metrics](images\\hostandapp.PNG)\n",
    "\n",
    "\n",
    "**Bonus Question: What is the difference between a Service and a Resource?**\n",
    "A service is a group of resources. Short answer: service is essentially the application and resource is essentially it's endpoints/methods.\n",
    "\n",
    "Here's my application's resource list:\n",
    "\n",
    "![resource list](images\\resources.png)\n",
    "\n",
    "and here's the service to which they all belong:\n",
    "\n",
    "![services](images\\services.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my call with Fahim, I already volunteered call center/suport desk metrics: call volume, hold time, etc. Anomoly detection would be helpful in addition to developing a more robust view of the enterprise.\n",
    "\n",
    "Here's a crazy idea that might just work:\n",
    "All on-call engineers track their location via smartphone or whatever is available. Datadog could alert the manager if too few engineers are within rapid-response range of the office. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
